% Load required themes and packages.
\documentclass{beamer}
\usepackage{mdframed}

\usetheme{Pittsburgh}
\usecolortheme{default}
\useinnertheme{default}
\useoutertheme{default}
\usefonttheme{structurebold}

% Import the necessary preamble for the document. 
\usepackage{../../proofsPrograms}

% Bibliography
\usepackage[style=numeric]{biblatex}
\addbibresource{../../proofsPrograms.bib} 
% In case of error: check the file path!
% the ../../ acts to jump back to files in path.
% Command line sequence:
%   pdflatex *filename* without .tex
%   biber *filename* without .bib
%   pdflatex *filename* without .tex

% Remove navigation bar
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[frame number]

% Definition format options. 
\newtheoremstyle{indentDefn}
{\topsep} % Space above
{\topsep} % Space below
{\it} % Body font
{2cm} % Indent amount
{\bf} % Theorem head font
{:} % Punctuation after theorem head
{0.5em} % Space after theorem head
{} % Theorem head spec

\theoremstyle{indentDefn} \newtheorem{defn}[]{Definition}

\title{Propositional Logic}
\author{MATH230}
\institute{Te Kura P\=angarau \\ Te Whare W\=ananga o Waitaha}
\date{}

% Document body starts here.
\begin{document}


% Title frame
\begin{frame}

  \titlepage

\end{frame}

% Table of contents page
\begin{frame}
  \frametitle{Outline}

  \tableofcontents

\end{frame}

\section{Arguments}

\begin{frame}
	\frametitle{Theorems}

	% The business of mathematicians is proving theorems.
	% What is a theorem? What does it mean to prove one? 

\end{frame}

\begin{frame}
  \frametitle{Arguments and Proofs}

	Analysis of the correctness of a theorem relies as much on the connective words as it does the technical definitions. 

\vspace{0.5cm}

{\bf Example: }
 
If $p$ divides $ab$, then $p$ divides $a$ or $p$ divides $b$.
  
\vspace{4cm}
% Content terms: 
% Connective terms: 

% Emphasis on the hypothetical claim. 

\end{frame}

\begin{frame}
	\frametitle{Arguments and Proofs}
  
	Analysis of the correctness of a theorem relies as much on the connective words as it does the technical definitions. 
  
  {\bf Example}
  
  If $a$ divides $b$ and $b$ divides $a$, then $a = b$ or $a = -b$. 

  \vspace{5cm}
  
  \end{frame}

  \begin{frame}
	\frametitle{Arguments and Proofs}
  
	Analysis of the correctness of a theorem relies as much on the connective words as it does the technical definitions. 
   
	{\bf Example}
	
	If $f(x)$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there exists a $c \in (a,b)$ such that $$f'(c) = \frac{f(b)-f(a)}{b-a}$$
  
  \end{frame}

\begin{frame}
	\frametitle{Connective Tissue}

	It is the connective words, as much as the mathematical content words, that we have to analyse when deciding whether these statements are correct; whether these are theorems.

	\vspace{0.5cm}

	{\bf Propositional logic formalises the structure of these connective words.}

\end{frame}

\begin{frame}
	\frametitle{Example: Natural Language}
	
	\begin{itemize}
		\item If Watson moves in with Holmes, then Holmes will be forever annoyed. Watson moved in with Holmes. Therefore, Holmes will be forever annoyed.
		\vspace{1cm}
		\item If Watson can trap Moriarty, then Holmes can. Holmes can't trap Moriarty. Therefore, Watson can't.
		\vspace{1cm}
		\item Either Holmes catches Moriarty or the world will fall into chaos. The world has fallen into chaos. Therefore, Holmes did not catch Moriarty.
		
	\end{itemize}	
\end{frame}

% Each of these examples are built up of declarative statements (propositions), one of which is singled out (by the therefore)  as a conclusion. The rest we call premises. 

\begin{frame}
	\frametitle{Argument}
	

	\begin{defn}
		An argument is a finite collection of declarative sentences (propositions), one of which is singled out as the conclusion, while the others are considered premises. 
	\end{defn}
		 
	\vspace{0.5cm}
	
	Premises are the evidence claiming to support the conclusion.	

\end{frame}

\begin{frame}
	\frametitle{Example: Natural Language}
	
	\begin{itemize}
		\item If Watson moves in with Holmes, then Holmes will be forever annoyed. Watson moved in with Holmes. Therefore, Holmes will be forever annoyed.

		\vspace{0.5cm}

		Let's break this up into premises and conclusion: 
		
		\vspace{5cm}
		
		% P1: If Watson moves in with Holmes, then Holmes will be forever annoyed. 
		% P2: Watson moved in with Holmes. 
		%  C: Holmes will be forever annoyed.
		
		% Ask questions about the structure of P1 and P2.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Propositional Structures}
	
	\begin{defn} 
		
		An atomic proposition has no propositional substructure.
		
	\end{defn}


	
	We saw above that some propositions do have extra structure: ``If... , then....'' and ``Either .... or ... '' and ``can't'' are important to the nature of the argument. 
	
	\vspace{0.5cm}
	
	Such connectives are used to join atomic propositions into compound propositions.	
\end{frame}

\begin{frame}
	\frametitle{Example: Natural Language}
	
	\begin{itemize}
		\item Either Holmes catches Moriarty or the world will fall into chaos. The world has fallen into chaos. Therefore, Holmes did not catch Moriarty.
		
		\vspace{0.5cm}
		
		Let's break this up into premises and conclusion and determine the atomic propositions.
		
		\vspace{5cm}
		
		% P1: If Watson moves in with Holmes, then Holmes will be forever annoyed. 
		% P2: Watson moved in with Holmes. 
		%  C: Holmes will be forever annoyed.
		
		% Ask questions about the structure of P1 and P2.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Moving Away from Natural Language}
	
	% Recall the example about numbers.
	% Recall the first lecture about the need to choose axioms. 
	
	It was hoped that mathematics could be written in such a precise manner that it could be routinely checked. Furthermore, it was thought that once mathematics was so formalised, that it could be shown consistent and complete; that is, not able to prove non-sense and able to prove (or refute) every statement. 

	\vspace{1cm}

	Toward this end mathematicians (Frege and those that followed him) chose to write mathematics in the language of logic: 

	\begin{itemize}
		\item Propositional Logic.
		\item First Order Predicate Logic.
	\end{itemize}
	
	
\end{frame}

\section{Propositional Logic}

\begin{frame}
	\frametitle{Propositional Connectives}
	
	To express the same syntactic structure of an argument without the ambiguities of a natural language we use capital (English) letters to denote atomic propositions, called \emph{propositional variables}. We use the following symbols to construct compound propositions: 
	
	\vspace{0.5cm}
	
	\begin{itemize}
		\item $\lnot$ : ``It is not the case that... '' or ``Not... ''
		\item $\land$ : ``Both... and ... '' 
		\item $\lor$ : ``Either... or ... ''
		\item $\rightarrow$ : ``If... , then ... ''
		\item $\leftrightarrow$ : `` ... if and only if ... ''
	\end{itemize}

	These symbols, the propositional connectives, play the role of the connective tissue in the statements given on previous slides. 
	
\end{frame}

\begin{frame}
	\frametitle{Example: NL to PL}
	\begin{itemize}
		\item If Watson can trap Moriarty, then Holmes can. Holmes Can't trap Moriarty. Therefore Watson can't.
	\end{itemize}
	
	\vspace{5cm}
	
	
\end{frame}	

\begin{frame}
	\frametitle{Grammar}
	
	Our language is further made up of \emph{well-formed formulae} which we define inductively as follows:	
	
	\vspace{0.5cm}
	\begin{defn}[Well-Formed Formulae]
	\begin{itemize}
		\item {\bf Atomic Formulae:} If $\alpha$ is a single propositional variable, then $\alpha$ is a wff.
		\item {\bf Negation:} If $\alpha$ is a wff, then $\lnot\alpha$ is a wff. 
		\item {\bf Binary Connective:} If $\alpha$ and $\beta$ are wff and $*$ is a binary connective, then $(\alpha * \beta)$ is a wff. 		
	\end{itemize}
	\end{defn}
	
	{\bf Notation:} We will refer to the totality of well-formed propositions as ``Prop'' and we will write ``$\alpha : \prop$'' to denote the fact that $\alpha$ is a well-formed proposition. 
	
\end{frame}

\begin{frame}
	\frametitle{Examples}
	Which of the following are wff in propositional logic?
	
	\begin{itemize}
		\item[1.] $A$
		\item[2.] $AB$
		\item[3.] $(A \rightarrow B)$
		\item[4.] $A \rightarrow B \rightarrow C$
		\item[5.] $((A \rightarrow B) \rightarrow C)$
		\item[6.] $\lnot Q$
		\item[7.] $A \lor Q$
		\item[8.] $A \rightarrow \lnot B \lor C$		
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Binding Conventions}
			
	{\bf Binding Conventions:} %If in doubt, then stick to using parentheses!
	\begin{itemize}
		\item $\lnot$ binds most tightly, 
		\item $\lor$ and $\land$ bind more tightly than $\rightarrow$,
		\item $\rightarrow$ binds more tightly than $\leftrightarrow$. 			
	\end{itemize}

	\vspace{0.2cm}

	{\bf Example:} Parse the wff $A \to \lnot B \lor C$.

	\vspace{2cm}

\end{frame}

\begin{frame}
	\frametitle{Binding Conventions}
			
	{\bf Binding Conventions:} %If in doubt, then stick to using parentheses!
	\begin{itemize}
		\item $\lnot$ binds most tightly, 
		\item $\lor$ and $\land$ bind more tightly than $\rightarrow$,
		\item $\rightarrow$ binds more tightly than $\leftrightarrow$. 			
	\end{itemize}

	To disambiguate $\land$ and $\lor$ we group terms from the left. In this way, we say that $\land$ and $\lor$ associate to the left. 

	{\bf Example:} Parse the wff $A \land B \land C$.

	\vspace{2cm}

\end{frame}

\begin{frame}
	\frametitle{Syntax Trees}

	Using the binding convention above allows for each well-formed formula to be parsed into a syntax tree. 

	\begin{table}[h]
		\centering
		\begin{tabular}{c c c}
			$A \land (B \lor C)$ \hspace{1.5cm} & $A \land \lnot B \land C$ \hspace{1cm} & $A \to B \lor C \land D$ \\
			\hline
		\end{tabular}
	\end{table}
	\vspace{4cm}
\end{frame}

\begin{frame}
	\frametitle{Hiding the Goods}

	By packing the statements: 
	
	\begin{center}
		``p divides ab'' or ``$f$ is continuous on $[a,b]$''
	\end{center}

	into a propositional variable $A$, we have lost a lot of information from the statement that we're trying to analyse. It's no longer about primes or continuous functions.

	\vspace{0.5cm}

	For now, we will focus on the connectives alone. Studying the structure of the argument, rather than the mathematics.

	\vspace{0.5cm}

	Later we will introduce more structure to our logic which will allow us to bring back the mathematical content. 

\end{frame}


	
\begin{frame}
  \frametitle{Example}
	
	``Thin is guilty,'' observed Watson, ``because either Holmes is right and the vile Moriarty is guilty, or he (Holmes) is wrong and Thin did the job; but those scoundrels are either both guilty or both innocent; and, as usual, Holmes is correct''.
	
	\vspace{5.5cm}


	\footnotesize{Example from Richard Jeffrey, \emph{Formal Logic: Its Scope and Limits}}

\end{frame}


\begin{frame}
	\frametitle{Argument Structure}
	\begin{center}
		$\begin{array}{ l c r }			
			& \textnormal{Proposition 1} & \\
			& \textnormal{Proposition 2} & \\
			& \vdots & \\
			& \textnormal{Proposition n} & \\
			\cline{1 - 3} 
			& \textnormal{Conclusion} & 	
		\end{array}$
	\end{center}

	\vspace{1cm}

{\bf Question:} What makes for a ``good argument''? What might we mean by a ``good argument''? What dos it mean for the conclusion to follow from the hypotheses? 
\end{frame}

\section{Natural Deductions}

\begin{frame}
	\frametitle{Truth and Proof}

	\begin{table}[h]
		\centering
		\begin{tabular}{c  p{0.7\textwidth}}
		Semantic & If the conclusion is ``true'' whenever all hypotheses are ``true'', then the conclusion is said to be a semantic consequence of the hypotheses. \\

		\vspace{0.2cm} & \\

		Syntactic & If there is a ``proof'' that the hypotheses ``unfold'' and ``combine'' towards the conclusion, then the conclusion is said to be a syntactic consequence of the hypotheses. 
		\end{tabular}
	\end{table}

	\vspace{0.5cm}

	{\bf Theorem:} $\gamma$ is a semantic consequence of $\Sigma$ if and only if $\gamma$ is a syntactic consequence of $\Sigma$ \cite{vDalen}. 	
\end{frame}

\begin{frame}
	\frametitle{Example}
	Provide a proof to show $$P \land (Q \land R), \ R \to T  \ \vdash \ P \land T$$

	\vspace{4cm}

	\begin{itemize}
		\item How are we to ``unfold'' or make use of hypotheses? 
		\item How are to obtain the conclusion? 
		\item What should such a proof look like? 
	\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{BHK Intepretation}
  % This is a philosophical ideal. But vague.
  % What should such algorithms look like?
  Brouwer, Heyting, and Kolmogorov proposed the following (inductive) interpretation of what it should mean to prove statements involving propositional connectives:

  \vspace{0.5cm}

  \begin{center}
    \begin{tabular}{p{1.5cm}p{8cm}}
      $P \land Q$ & to prove a conjunction we must provide both a proof of P and a proof of Q. \\      
      $P \to Q$ & to prove an implication we must provide an algorithm for turning a proof of P into a proof of $Q$.\\
      $P \lor Q$ & to prove a disjunction we must provide either a proof of P or a proof of Q. \\
      $\lnot P$ &  to prove a negation we must provide an algorithm that turns a proof of P into a proof of $\bot$.
    \end{tabular}
  \end{center}

  \vspace{0.5cm}
  % The negation interpretation is not clear at the moment. But we will clarify that in time. 

  This presentation is taken from the Standford Encyclopedia of Philosophy article by Bridges, Palmgren, and Ishihara \cite{sep-mathematics-constructive}.
\end{frame}

\begin{frame}
  \frametitle{Natural Deduction Calculus}

  We are going to develop a proof method that is inline with the BHK interpretation of the logical connectives. This proof method was first presented by Gerhard Gentzen. As such it is often referred to as the Gentzen Calculus. 

  \vspace{0.5cm}

  This method will develop proofs by unfolding their hypotheses in a manner consistent with the BHK.
\end{frame}

\begin{frame}
	\frametitle{Example}
	Provide a proof to show $$P \land (Q \land R), \ R \to T  \ \vdash \ P \land T$$

	% Following the BHK this will consist of two proofs (i) one of P and (ii) one of T.

	\vspace{6cm}
	
\end{frame}

\begin{frame}
	\frametitle{Natural Deduction Calculus}

	{\bf Example:} What would it require to deduce $A \land B$ in the course of a proof? Use the BHK!

	\pause
	\begin{center}
		$\begin{array}{ c  c  c }			
			\infer[\land I]{A \land B}
				{\begin{array}{ c  c  c }			
					\Sigma_{1} & & \Sigma_{2} \\
					\mathcal{D}_{1} & & \mathcal{D}_{2} \\
					A & & B 
				\end{array}}
		\end{array}$
	\end{center}

	If we have a deduction for $A$ and a deduction for $B$, then together we should consider those deductions a proof for $A \land B$.	
  % Deductions are thus built inductively from hypotheses. 
\end{frame}

\begin{frame}
	\frametitle{$\land$ Elimination}
	
	{\bf Example:} Suppose $A\land B$ were a premise in a proof. What can we conclude from such a premise?
	
	\pause
	\vspace{2cm}
	
	\begin{center}
		$\begin{array} { c c c }
		\infer[\land E_{L}]{A}
			{\begin{array}{c} \Sigma \\ \mathcal{D} \\ A \land B \end{array}}
		& \hspace{3cm} &
		\infer[\land E_{R}]{B}
			{\begin{array}{c} \Sigma \\ \mathcal{D} \\ A \land B \end{array}}
		\end{array}$
	\end{center}	
\end{frame}

\begin{frame}
	\frametitle{Example: Commutativity of $\land$}
	
	Provide a proof to show $$A \land B \vdash B \land A$$
		
	\vspace{6cm}
	
\end{frame}

\begin{frame}
	\frametitle{Example: Idempotence of $\land$}
	
	Provide a proof to show $$A \land A \vdash A$$
	
	\vspace{6cm}
	
\end{frame}

\begin{frame}
	\frametitle{Deductions}
	
	\begin{defn} We define deductions (or derivations, or proofs) inductively according to the following rules: 
	
	\begin{itemize}
		\item For each formula $\alpha$,
		$$ \alpha $$
		is a deduction with conclusion $\alpha$ and premises $\{\alpha\}$.
		
		\item From a given deduction, an application of a \textit{rule of inference} yields a new deduction.
		
		\item Anything that is not a deduction by virtue of the above is \emph{not} a deduction. 
	\end{itemize}

	\end{defn}
	
	\vspace{0.3cm}

	If there exists a deduction $^\Sigma_{\alpha}\mathcal{D}$ of $\alpha$ from $\Sigma$, then we say $\alpha$ is a \textit{syntactic consequence of }(\textit{derivable from}, or \textit{provable from}) $\Sigma$ and denote this $\Sigma \vdash \alpha$. 
	
	
\end{frame}

\begin{frame}
	\frametitle{Propositional Calculus}
	
	If we follow this idea for all of the logical connectives in propositional logic, then we can develop a method for writing proofs based on the \textit{syntactic} structre of the logical connectives alone. We call this method of proof \emph{Natural Deduction} and we are following Gerhard Gentzen's notation \cite{vDalen,thompson}.
	
	\vspace{0.3cm}
	
	We need to know how to (i) deduce and (ii) conclude from, each logical connective. In other words, for each logical connective we need to develop rules for introducing the logical connective and eliminating the logical connective.
	
	\vspace{0.3cm}
	
	So we will spend some time writing down the {\bf Rules of Inference} for our logical connectives. 
	
	% Note that the LEAN notes follows the same notation. However the Logic Matters notes do not: still worth reading! 
\end{frame}



\begin{frame}
	\frametitle{Hypothetical Reasoning}
	
	In mathematics we often prove statements of the following hypothetical form: ``If ..., then ...'' 
	
	\vspace{0.3cm}
	
	{\bf Example:} If $f$ is differentiable at $x_{0}$, then it is continuous at $x_{0}$.
	
	\vspace{0.3cm}

  	{\bf Proof:} ``Let $f$ be a differentiable function...''

  	\vspace{0.3cm}
	
	The proof of an implication will assume the hypothesis of differentiability and show that it implies continuity. In order to prove an implication $P \to Q$ the proof starts by assuming we know $P$ and then using that to tell us about $Q$. 
	
	\vspace{0.3cm} 
	
	The conclusion is the entire implication, not just continuity. 
	
	\vspace{1cm}
	
\end{frame}

\begin{frame}
\frametitle{Example: Hypothetical Reasoning}

	If $p | a$ and $p | (a + b)$, then $p | b$.
	%Use this example to highlight the nature of a hypothetical argument. 

\vspace{6cm}	

\end{frame}

\begin{frame}
	\frametitle{Example: Hypothetical Reasoning}

	\begin{center}
		$\begin{array}{c}
			\infer[\land E_{L}]{B}
				{\infer[\land E_{L}]{B \land C}
					{\infer[1]{A \land (B \land C)}
						{}}}
		\end{array}$
	\end{center}

	\vspace{4cm}

	{\bf Question:} What does the proof prove?
	% It does not prove B outright. 
	% It shows that B follows from the assumption A \land (B \land C)
	% This proof is not happening in a vaccuum. 

\end{frame}

\begin{frame}
  \frametitle{$\rightarrow$ Introduction}

	If $^{\Sigma}_{\beta}\mathcal{D}$ is a deduction of $\beta$ from $\Sigma$, then

	\begin{center}		
		$\begin{array}{c}		
			\infer[\rightarrow I]{\alpha \rightarrow \beta}
				{\begin{array}{c} \Sigma \cup \{\cancel{\alpha}\} \\ \mathcal{D} \\ \beta \end{array}}
		\end{array}$
	\end{center}

	is a deduction of $\alpha \rightarrow \beta$ from hypotheses $\Sigma \backslash \{\alpha\}$.
	
	\vspace{1.5cm} 
	
	{\bf Note: }As the assumption $\alpha$ is struck out after this deduction, we are free to use $\alpha$ \emph{even if it is not in} $\Sigma$ when using implication introduction. 

\end{frame}

\begin{frame}
	\frametitle{$\rightarrow$ Elimination (MP)}
	
	If $^{\Sigma_{1}}_{\alpha \rightarrow \beta}\mathcal{D}_{1}$ and $^{\Sigma_{2}}_{\alpha}\mathcal{D}_{2}$ are deductions, then
	
	\vspace{0.5cm}
	
	\begin{center}
		$\begin{array}{c}		
			\infer[\rightarrow E]{\beta}{\begin{array}{c} \Sigma_{1} \\ \mathcal{D}_{1} \\ \alpha \rightarrow \beta \end{array} \quad & \begin{array}{c} \Sigma_{2} \\ \mathcal{D}_{2} \\ \alpha \end{array}}	
		\end{array}$
	\end{center}
	
	is a deduction of $\beta$ from $\Sigma_{1} \cup \Sigma_{2}$.
	% Note that you can label the  step in the proof as MP instead. 
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	Show $P \rightarrow Q, \ Q \rightarrow R \ \vdash \ P \rightarrow R$
	
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{Deduction Theorem}	
	
	{\bf Theorem:} $\Sigma \vdash \alpha \rightarrow \beta$ if and only if $\Sigma \cup \{\alpha\} \vdash \beta$
	
	\vspace{0.5cm}
	
	{\bf Proof} 
	
	\vspace{5cm}
	
	% Note: this is a metalogical statement about possible deductions, rather than any specific deduction.
	
	% This is not so interesting in the deduction method that we've built up. In other systems this is a real theorem. With the rules of inference as we have, this should not be surprising. For us it is just implication introduction and elimination. 
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	Show that $P \rightarrow (\lnot S\rightarrow L), \hspace{1em} P\rightarrow \lnot S, \hspace{1em} P \vdash L$

	\vspace{7cm}
	
	% \pause	
	% % Write this out by hand in the lecture!
	% \begin{center}
	% 	$\begin{array}{c}
	% 		\infer[MP]{L}{\infer[MP\hspace{1cm}]{\lnot S \rightarrow L}{ P \rightarrow (\lnot S\rightarrow L) & P } 
	% 						&
	% 					  \infer[MP]{\lnot S}{ P \rightarrow \lnot S & P }
	% 				  	 	}
	% 	\end{array}$		
	% \end{center}
\end{frame}

\begin{frame}
  \frametitle{Currying}

  Show $(A \land B) \to C \vdash A \to (B \to C)$
  
  \vspace{6cm}

\end{frame}


\begin{frame}
	\frametitle{Example}

	Show $A \to B, A \to C, A \ \vdash B \land C$
	
	\vspace{6cm}

\end{frame}

\begin{frame}
	\frametitle{$\lor$ Introduction}
	
	If $^{\Sigma}_{\alpha}\mathcal{D}$ is a derivation of $\alpha$ from $\Sigma$, then 
	
	\begin{center}
		
	$\begin{array}{ c c c }	
		\begin{array}{ c c }		
			\begin{array}{c}		
				\infer[\lor I_{R}]{\alpha \lor \beta}
				{\begin{array}{c} \Sigma \\ \mathcal{D} \\ \alpha \end{array}}
			\end{array}
		\end{array}
		
		&
		
		&
		
		\begin{array}{ c c }		
			\begin{array}{c}		
				\infer[\lor I_{R}]{\beta \lor \alpha}
				{\begin{array}{c} \Sigma \\ \mathcal{D} \\ \alpha \end{array}}
			\end{array}
		\end{array}		
		
		
	\end{array}$
	\end{center}
	
	are derivations of $\alpha \lor \beta$ and $\beta \lor \alpha$ from $\Sigma$. 
	
	\vspace{1cm}
	
	{\bf Note:} We are free to choose $\beta$ as, if we know $\alpha$ to be the case, then $\alpha \lor \beta$  is necessarily the case \emph{for any }$\beta$. 
	
\end{frame}

\begin{frame}
	\frametitle{$\lor$ Elimination}
	
	If $^{\Sigma_{1}}_{\alpha\lor\beta}\mathcal{D}_{1}$, $^{\Sigma_{2}}_{\alpha\rightarrow\gamma}\mathcal{D}_{2}$, and $^{\Sigma_{2}}_{\beta\rightarrow\gamma}\mathcal{D}_{3}$ are derivations, then
	
	\begin{center}
	$\begin{array}{ c }
	
		\infer[\lor_{E}]{\gamma}
			{
			\begin{array}{c} \Sigma_{1}  \\ \mathcal{D}_{1} \\ \alpha\lor\beta \end{array}
			 & 
			\begin{array}{c} \Sigma_{2}  \\ \mathcal{D}_{2} \\ \alpha\rightarrow\gamma \end{array}				
			 & 
			\begin{array}{c} \Sigma_{2}  \\ \mathcal{D}_{2} \\ \beta\rightarrow\gamma \end{array}				 
			}	
	
	\end{array}$
	\end{center}
	
	is a derivation of $\gamma$ from $\Sigma_{1}\cup\Sigma_{2}\cup\Sigma_{3}$.
	
	\vspace{0.5cm}
	
	{\bf Note:} You can't remove one of the arguments from a disjunction. Knowledge of $\alpha \lor \beta$ is not sufficient to conclude either $\alpha$ or $\beta$ alone. Following the BHK, a proof of $\alpha \lor \beta$ is either a proof of $\alpha$ or a proof of $\beta$, but without a record of which case we are in we can't assume either way. This means we need to account for both possibilities. 
	
\end{frame}



\begin{frame}
  \frametitle{Example}

	Show $A \lor B, \ (A \lor C)\rightarrow D, \ B \rightarrow D  \ \vdash D$
	
	\vspace{7cm}



\end{frame}

\begin{frame}
  \frametitle{Example}

	Show $A \lor (B \land C) \ \vdash (A \lor B) \land (A \lor C)$
	
	\vspace{7cm}



\end{frame}

\begin{frame}
	\frametitle{Common Mistake (!)}

	% Incorrect or elimination. 
	% Can use this to show A \vdash B... Prove anything!
	% Elimination rule can't be the same as and... it's a different connective!

	% \begin{center}
	% 	$\begin{array}{c}
	% 		\infer[\lor E (!)]{B}
	% 			{\infer[\lor I]{A \lor B}
	% 				{B}}
	% 	\end{array}$
	% \end{center}

\end{frame}

\begin{frame}
  \frametitle{Positive Minimal Logic}

    The calculus developed so far with the introduction and elimination rules of the three connectives $\land, \lor,$ and $\to$ is called positive minimal logic. 

    \vspace{0.5cm}

    Many theorems can be (stated and) proved with these rules of inference alone. 

    \vspace{0.5cm}

    But there are theorems that can't be proved using these rules alone. 

\end{frame}



\begin{frame}
	\frametitle{Falsum}
	
	We introduce the logical constant $\bot$ (falsum or absurdity) to define the syntactic form of the $\lnot$ connective. We make the following definition: 
	
	$$\lnot \alpha:= \alpha \rightarrow \bot$$
	
	\vspace{0.5cm}
	
	\begin{center}
		$\begin{array} { c c c }
		
		\infer[MP]{\bot}{\alpha \quad & \alpha \rightarrow \bot}
		
		& \hspace{3cm} &
		
		\infer[\rightarrow I]{\alpha \rightarrow \bot}{\begin{array}{c} 
			\hline \cancel{\alpha} \\
			\mathcal{D} \\ 
			\bot			
		\end{array}}		
		
		\end{array}$
	\end{center}
	
	\vspace{1cm}
	
	Falsum $\bot$ is an atomic proposition which is to be thought of as denoting ``absurdity'' or ``contradiction''. 
	
\end{frame}

\begin{frame}
	\frametitle{Example: Modus Tollens}	
	
	Show $A \rightarrow B, \ \lnot B \ \vdash \lnot A$
	
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{Contradiction Implies Absurdity}
	
	Show $A \land \lnot A \ \vdash \ \bot$
	
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{Minimal Logic}
	
	Together the rules of inference that we've given so far define \emph{minimal} logic. They include much, but not all, of the logical inferences that practising mathematicians might use in a proof. What they do include is uncontroversial.
	
	\vspace{0.5cm}
	
	However, it is not universally agreed as to how minimal logic should be extended. There are philosophical differences among mathematicians and logicians about what other rules of inference should be included. 
	
	\begin{itemize}
		\item Intuitionistic logic
		\item Classical logic 
		\item Modal logic		
	\end{itemize}	
\end{frame}

\begin{frame}
  \frametitle{What's Missing?}

	What should be done if the hypotheses yield a contradiction? 

	\begin{center}
		$\begin{array}{c}
		\infer[?]{?}{\begin{array}{c} \Sigma \\ \mathcal{D} \\ \bot \end{array}}
		\end{array}$
	\end{center}

	Should we be able to prove the following? 
	\begin{itemize}
		\item[] $\vdash \ P \lor \lnot P$
		\item[] $\lnot \lnot P \ \vdash \ P$
		\item[] $\vdash \ (P \to Q) \lor (Q \to P)$
	\end{itemize}

	Affirmative answers to these questions require further rules of inference. 

\end{frame}



\begin{frame}
	\frametitle{Ex Falso Sequitur Quadlibet}
	
	So far, we have not made much mention of how to deal with the derivation of $\bot$ absurdity. Indeed, it has no introduction rule. 
	
	\vspace{0.5cm}
		
	If $^{\Sigma}_{\bot}\mathcal{D}$ is a deduction of $\bot$ from $\Sigma$, then
	
	\begin{center}		
		$\begin{array}{c}		
		\infer[\text{XF}]{\alpha}
		{\begin{array}{c} \Sigma \\ \mathcal{D} \\ \bot \end{array}}
		\end{array}$
	\end{center}

	is a derivation of $\alpha$ from the assumptions $\Sigma$.
	
	\vspace{0.5cm}
	
	\emph{Anything you want follows from a falsehood.} % Recall this is semantically valid. 
	 
\end{frame}

\begin{frame}
	\frametitle{Disjunctive Syllogism}
	
	Show $A \lor B, \lnot B \ \vdash A$
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{XF as Null Disjunction}

	% Idea from Pfenning, again. 
	Disjunction has two introduction rules; both of which need to be taken into account when eliminating a disjunction. 

	Following this one can argue that since $\bot$ has no introduction rules, there is nothing to take into account of when eliminating $\bot$ and hence XF allows one to conclude anything. All possible cases (i.e. all zero of them) lead to $P$, therefore we may conclude $P$ follows from Falsum. 

	\vspace{3cm}
	% Draw the rules for disjunction. Do the analogy with the rules for falsum. 

\end{frame}

\begin{frame}
  \frametitle{Intuitionistic Logic}

	Ex Falso Quodlibet extends the class of theorems provable in the natural deduction calculus. It is the logic of intuitionists and constructivists; mathematicians who believe proofs should have computational content.
	
	\begin{center}
  		Minimal Logic + Ex Falso = Intitionistic Logic
	\end{center}

	However, there are classically valid sequents, such as the LEM, which are not derivable in the intuitionistic calculus. 

	\vspace{4cm}

\end{frame}

\begin{frame}
	\frametitle{Double Negation Elimination}
	
	Show $\lnot\lnot A \vdash A$ 
	\vspace{5cm}
	
	\pause
	% LaTeX the derivation here. 
	
	{\bf Ex falso does not give us a proof.} In fact we have shown the following: $\{\lnot\lnot A, \lnot A\} \vdash A$.
	
\end{frame}



\begin{frame}
	\frametitle{Summary}

	We have built the propositional calculus up in steps: 

	\begin{itemize}
		\item Positive minimal logic, 
		\item Minimal logic, 
		\item Intuitionistic logic.
	\end{itemize}

	\vspace{0.5cm}

	One can show, using non-classical semantics, that these logics are unable to prove some theorems that are fundamental to much of mathematics. 

	\vspace{0.3cm}

	We will consider one more mode of reasoning. % Ask class for suggestions. What is missing from 120? 

\end{frame}

\begin{frame}
	\frametitle{Example}

	If $x,y > 0$, then $x + y > 0$. 

	\vspace{6cm}

\end{frame}

\begin{frame}
	\frametitle{Reductio Ad Absurdum}
	
	If $^{\Sigma}_{\bot}\mathcal{D}$ is a deduction of $\bot$ from $\Sigma$, then
	
	\begin{center}		
		$\begin{array}{c}		
		\infer[\RAA]{\alpha}
		{\begin{array}{c} \Sigma \cup \{\cancel{\lnot\alpha}\} \\ \mathcal{D} \\ \bot \end{array}}
		\end{array}$
	\end{center}
	
	is a derivation of $\alpha$ from the assumptions $\Sigma \backslash\{\lnot\alpha\}$.
	
	\vspace{0.5cm}
	
	If absurdity follows from $\lnot\alpha$, then we may conclude $\alpha$ {\bf and discharge $\lnot \alpha$ from our assumptions.}
	 
\end{frame}

\begin{frame}
	\frametitle{Double Negation}	
	
	Show $\lnot\lnot A \vdash A$
	\vspace{7cm}	
	
	% Notice the difference if Ex Falso is used - then we prove something very different!
\end{frame}

\begin{frame}
	\frametitle{Law of Excluded Middle}
	
	Show $\vdash A \lor \lnot A$
	\vspace{7cm}
	
	% Notice the difference if Ex Falso is used - then we prove something very different!	
\end{frame}

\begin{frame}
  \frametitle{Classical Logic}

	The class of theorems one can prove increases with the addition of RAA. In fact, the class of theorems provable in classical logic includes all theorems of intuitionistic logic. 

	\begin{center}
  		Minimal Logic + RAA = Classical Logic 	
	\end{center}

	One can derive the Ex Falso rule of inference using RAA. 

	\vspace{4cm}

\end{frame}

\begin{frame}
	\frametitle{Common Misconception}

	Many proofs that claim to use reductio ad absurdum are really \emph{refutations by contradiction.}

	\begin{itemize}
		\item Irrationality of $\sqrt{2}$
		\item Infinitude of primes
		\item No smallest positive rational number
	\end{itemize}

	\vspace{3cm}

	These proofs just use implication introduction to prove a negation. 
	% Splitting hairs; but that is what we are doing! 
	% Probably lost on most students at this point. 
	% When we are considering the methods so carefully we 
	% should expect to identify and untangle ideas that 
	% we had conflated. 

\end{frame}

\begin{frame}
	\frametitle{Example}
	Show $A \rightarrow B, \hspace{0.2cm} A \rightarrow \lnot B \ \vdash \lnot A$
	\vspace{7cm}
	
	
	% The last step can be achieved by RAA... but it can also be seen as implication introduction. It's minimal!
  	% Don't roll in the big guns if you don't have to... it's not in good taste. 
	
\end{frame}



\begin{frame}
	\frametitle{Derived Rules of Inference}
	
	Proofs can be simplified by using results already proved. You may, in the course of a proof, use any result that has been proven in class or previously in a tutorial. However, when substituting previous proofs, you must bring all of the premises with the conclusion. 
	
	\vspace{0.5cm}
	
	We have already seen this with the use of \emph{modus tollens} (MT) in some examples.
	
	\vspace{0.5cm} 
	
	Making use of (an instance of) LEM instead of RAA can make proofs more straight forward. 

	\vspace{0.5cm}

	This can help keep proofs manageable and neat. 

\end{frame}

\begin{frame}
	\frametitle{Example: Substituting LEM}
	
	%Earlier we showed $\vdash \alpha \lor \lnot\alpha$ so we can call on this theorem when ever it may help us.
	Show $A \rightarrow B \vdash \lnot A \lor B$
	
	\vspace{7cm}
		
	
\end{frame}

\begin{frame}
  \frametitle{Intuitionistic to Classical}

  	\begin{center}
	Classical = IL + RAA = IL + LEM = IL + DNE
	\end{center}

	\vspace{0.3cm}

    We described the passage from intuitionistic logic to classical by the addition of the RAA rule of inference. We can get a logic of equivalent power in a number of ways. 

    \vspace{0.3cm}

    One could declare for each $P$,  $P \lor \lnot P$ as a theorem. 

    \vspace{0.3cm}

    One could add a double negation elimination rule of inference. 

    \vspace{0.3cm}

    Adding any of these to minimal logic gives you the same set of theorems as classical logic as we defined it above. 

	{\bf See tutorial to prove this.}

\end{frame}

\begin{frame}
  \frametitle{Departure from BHK}

    Notice that the addition of RAA has forced us to lose the BHK interpretation of our proofs. For each proposition $P$ LEM is a theorem: 

    $$ \vdash P \lor \lnot P$$

    The BHK asserts a proof of $A \lor B$ must consist either of a proof of $A$ or a proof of $B$. But the classical proof of $\vdash P \lor \lnot P$ does not contain that information; it does not tell us which of $P$ or $\lnot P$ is provable. 

	\vspace{0.5cm}

	The inclusion of RAA allows for proofs of some apparently harmless theorems like DNE and RAA. However this power is not without its consequences...

\end{frame}

\begin{frame}
	\frametitle{Example}
	Provide a proof to show $$ \vdash (A \to B) \lor (B \to A)$$
	\vspace{6cm}
	
	% We get more theorems... but be careful what you wish for...
	% RAA allows for some strange theorems...
	% This is probably not implication in the sense you have in mind... 
	
\end{frame}

\begin{frame}
	\frametitle{Dealer's Choice}

	Choice of logic (i.e. rules of inference) is upto the logician/mathematician. Therefore, we should be more specific when we assert one Prop is a syntactic consequence of a set $\Sigma$. This is a \emph{relative} notion and so one should quote the logic used in the derivation. 

	\vspace{4cm}

	% Give examples. Highlighting the requirement for more RoI
	% We don't study the tools - semantics - required to show that proofs don't exist in certain logics. 

\end{frame}

\begin{frame}
  \frametitle{Logical Equivalence}

	\begin{defn}We say well-formed formulae are \textit{syntactically equivalent} if both 
	$$ \alpha \vdash \beta \quad \textnormal{and} \quad \beta \vdash \alpha$$\end{defn}
	
	\vspace{0.5cm}
	
	{\bf Examples} 
	\begin{itemize}
		\item $A \lor B \dashv \vdash B  \lor  A$
		\item $A \rightarrow B \ \dashv \vdash \ \lnot A \lor B$
	\end{itemize}

	Logical equivalences may be stated \emph{with respect to a logic}.

\end{frame}

\begin{frame}
	\frametitle{Theorems}
	\begin{defn} We say a well-formed formula $\alpha$ is a theorem if there exists a natural deduction $\mathcal{D}$ from no assumptions i.e. $\Sigma = \emptyset$ and we denote this as $\vdash \alpha$. \end{defn}
	
	\vspace{0.5cm}
	
	{\bf Example:} Law of the Excluded Middle
	
	\vspace{0.5cm}
	
	{\bf Example:} $\vdash A \rightarrow (B \rightarrow A)$
	
	\vspace{1cm}
	
	{\bf Note:} This should be stated \emph{with respect to a logic.}	
\end{frame}

\begin{frame}
	\frametitle{Different Logics}

	When we say one logic is weaker relative to another, what we are saying is that the set of theorems is a subset of the other. 

	\vspace{0.5cm}

	Reference to ``classical logic'' as a whole might be referring to the collection of the rules of inference, or it might be referring to the collection of all theorems of classical logic. 

	\vspace{0.5cm}

	\begin{center}
	Positive Minimal $\subset$ Minimal $\subset$ Intuitionistic $\subset$ Classical
	\end{center}

\end{frame}


%%%% Not sure what to say here; if anything. 
% \begin{frame}
% 	\frametitle{Classical Going too Far?}

% 	% Some comments about why classical logic may not be as obviously better as it seems with DNE and LEM etc. 

% 	% Fails to distinguish propositions. Conflates propositions

% 	% Is a coarser view on propositions. 

% 	% ... Doesn't have as clear a computational interpretation. 

% \end{frame}

\begin{frame}
	\frametitle{Example of Equivalence}	
	
	If $\alpha$ and $\beta$ are syntactically equivalent, then $\vdash \alpha \leftrightarrow \beta$. 
	
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{Equivalence of Theorems}
	
	If $\vdash \alpha$ and $\vdash \beta$, then $\vdash \alpha \leftrightarrow \beta$
	
	\vspace{7cm}
	
\end{frame}

\begin{frame}
	\frametitle{Further Reading}
	
    \printbibliography  
	
\end{frame}
\end{document}
